name: Process Download Analytics

on:
  schedule:
    # Run every 6 hours to process analytics data
    - cron: '0 */6 * * *'
  
  # Also run when new analytics issues are created
  issues:
    types: [opened]
  
  # Allow manual trigger for testing
  workflow_dispatch:

permissions:
  contents: write
  issues: read
  pull-requests: write

jobs:
  process-analytics:
    runs-on: ubuntu-latest
    if: github.repository == 'davila7/claude-code-templates'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Process Analytics Data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create analytics processing script
          cat > process_analytics.js << 'EOF'
          const fs = require('fs');
          const path = require('path');

          async function processAnalytics() {
            const token = process.env.GITHUB_TOKEN;
            const owner = 'davila7';
            const repo = 'claude-code-templates';
            
            console.log('🔍 Fetching analytics issues...');
            
            // Fetch analytics issues from GitHub API
            const response = await fetch(`https://api.github.com/repos/${owner}/${repo}/issues?labels=📊 analytics,download-tracking&state=all&per_page=100`, {
              headers: {
                'Authorization': `token ${token}`,
                'Accept': 'application/vnd.github.v3+json'
              }
            });
            
            if (!response.ok) {
              throw new Error(`Failed to fetch issues: ${response.status}`);
            }
            
            const issues = await response.json();
            console.log(`📊 Found ${issues.length} analytics issues`);
            
            // Process the analytics data
            const analytics = {
              total_downloads: 0,
              downloads_by_type: {
                agent: 0,
                command: 0,
                mcp: 0,
                template: 0,
                'health-check': 0,
                analytics: 0
              },
              downloads_by_component: {},
              downloads_by_date: {},
              last_updated: new Date().toISOString(),
              data_points: issues.length
            };
            
            // Parse each issue's analytics data
            for (const issue of issues) {
              try {
                // Extract JSON data from issue body
                const jsonMatch = issue.body.match(/```json\s*\n([\s\S]*?)\n```/);
                if (!jsonMatch) continue;
                
                const data = JSON.parse(jsonMatch[1]);
                
                // Count total downloads
                analytics.total_downloads++;
                
                // Count by type
                const type = data.component_type || 'unknown';
                if (analytics.downloads_by_type[type] !== undefined) {
                  analytics.downloads_by_type[type]++;
                }
                
                // Count by component name
                const component = data.component_name || 'unknown';
                if (!analytics.downloads_by_component[component]) {
                  analytics.downloads_by_component[component] = 0;
                }
                analytics.downloads_by_component[component]++;
                
                // Count by date
                const date = data.timestamp ? data.timestamp.split('T')[0] : 'unknown';
                if (!analytics.downloads_by_date[date]) {
                  analytics.downloads_by_date[date] = 0;
                }
                analytics.downloads_by_date[date]++;
                
              } catch (error) {
                console.log(`⚠️ Failed to parse issue #${issue.number}: ${error.message}`);
              }
            }
            
            // Sort components by download count (top 20)
            const topComponents = Object.entries(analytics.downloads_by_component)
              .sort(([,a], [,b]) => b - a)
              .slice(0, 20)
              .reduce((obj, [key, value]) => {
                obj[key] = value;
                return obj;
              }, {});
            
            analytics.downloads_by_component = topComponents;
            
            // Sort dates and keep last 30 days
            const sortedDates = Object.entries(analytics.downloads_by_date)
              .sort(([a], [b]) => new Date(b) - new Date(a))
              .slice(0, 30)
              .reduce((obj, [key, value]) => {
                obj[key] = value;
                return obj;
              }, {});
            
            analytics.downloads_by_date = sortedDates;
            
            console.log('📈 Analytics Summary:');
            console.log(`  Total Downloads: ${analytics.total_downloads}`);
            console.log(`  Agents: ${analytics.downloads_by_type.agent}`);
            console.log(`  Commands: ${analytics.downloads_by_type.command}`);
            console.log(`  MCPs: ${analytics.downloads_by_type.mcp}`);
            console.log(`  Templates: ${analytics.downloads_by_type.template}`);
            
            return analytics;
          }

          // Main execution
          processAnalytics()
            .then(analytics => {
              // Write analytics data to file
              const analyticsDir = path.join(process.cwd(), 'docs', 'analytics');
              if (!fs.existsSync(analyticsDir)) {
                fs.mkdirSync(analyticsDir, { recursive: true });
              }
              
              const analyticsFile = path.join(analyticsDir, 'download-stats.json');
              fs.writeFileSync(analyticsFile, JSON.stringify(analytics, null, 2));
              
              console.log(`✅ Analytics data written to ${analyticsFile}`);
              console.log(`📊 Processed ${analytics.data_points} data points`);
            })
            .catch(error => {
              console.error('❌ Analytics processing failed:', error);
              process.exit(1);
            });
          EOF

          # Run the analytics processing
          node process_analytics.js

      - name: Create Pull Request with Analytics Data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          if [ -f "docs/analytics/download-stats.json" ]; then
            git add docs/analytics/download-stats.json
            
            if git diff --staged --quiet; then
              echo "📊 No changes in analytics data"
            else
              # Create a new branch for the analytics update
              BRANCH_NAME="analytics-update-$(date +%Y%m%d-%H%M%S)"
              git checkout -b "$BRANCH_NAME"
              
              git commit -m "📊 Update download analytics data

              - Processed analytics issues for download tracking
              - Updated component download statistics
              - Generated by GitHub Actions workflow

              🤖 Automated analytics update"
              
              git push origin "$BRANCH_NAME"
              
              # Create pull request using GitHub CLI
              gh pr create \
                --title "📊 Automated Analytics Update $(date +%Y-%m-%d)" \
                --body "## Automated Analytics Data Update

              This pull request contains updated download analytics data processed from GitHub issues.

              ### Changes Made
              - 📊 Updated download statistics in \`docs/analytics/download-stats.json\`
              - 📈 Processed latest analytics issues for tracking
              - 🔄 Generated automatically by GitHub Actions

              ### Summary
              The analytics processor has collected and processed download tracking data from GitHub issues labeled with \`📊 analytics\` and \`download-tracking\`.

              **Safe to merge** - This only updates analytics data files.

              🤖 Generated automatically by GitHub Actions" \
                --head "$BRANCH_NAME" \
                --base main
              
              echo "✅ Analytics data committed and PR created"
            fi
          else
            echo "⚠️ Analytics file not found"
          fi

      - name: Analytics Summary
        run: |
          if [ -f "docs/analytics/download-stats.json" ]; then
            echo "📊 Analytics Processing Complete!"
            echo "====================================="
            
            # Extract key metrics using jq if available, otherwise use basic parsing
            if command -v jq &> /dev/null; then
              echo "📈 Download Summary:"
              jq -r '.downloads_by_type | to_entries[] | "  \(.key): \(.value)"' docs/analytics/download-stats.json
              echo ""
              echo "🏆 Top Components:"
              jq -r '.downloads_by_component | to_entries[] | select(.value > 0) | "  \(.key): \(.value) downloads"' docs/analytics/download-stats.json | head -10
            else
              echo "📁 Analytics file created successfully"
              echo "📊 Data available at: docs/analytics/download-stats.json"
            fi
          else
            echo "❌ Analytics processing may have failed"
          fi